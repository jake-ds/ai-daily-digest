# AI Daily Digest - 2026-01-15

## 오늘의 하이라이트

- [Information Capacity: Evaluating the Efficiency of Large Language Models via Text Compression](https://arxiv.org/abs/2511.08066)
  > arXiv:2511.08066v5 Announce Type: replace 
Abstract: Recent years have witnessed the rapid advancements of large language models (LLMs) and their expa...
- [ROSS: RObust decentralized Stochastic learning based on Shapley values](https://arxiv.org/abs/2411.00365)
  > arXiv:2411.00365v2 Announce Type: replace 
Abstract: In the paradigm of decentralized learning, a group of agents collaborate to learn a global model ...
- [Stop treating LLM context as a linear chat: We need a Context-Editing IDE for serious engineering and professional project development](https://www.reddit.com/r/LocalLLaMA/comments/1qd67i3/stop_treating_llm_context_as_a_linear_chat_we/)
  > <!-- SC_OFF --><div class="md"><p>Editing an image is purely cosmetic, but managing context is structural engineering. Currently, we are forced into a...

## AI 연구

- [Towards Specialized Generalists: A Multi-Task MoE-LoRA Framework for Domain-Specific LLM Adaptation](https://arxiv.org/abs/2601.07935)
- [Enriching Semantic Profiles into Knowledge Graph for Recommender Systems Using Large Language Models](https://arxiv.org/abs/2601.08148)
- [BenchOverflow: Measuring Overflow in Large Language Models via Plain-Text Prompts](https://arxiv.org/abs/2601.08490)
- [ExpSeek: Self-Triggered Experience Seeking for Web Agents](https://arxiv.org/abs/2601.08605)
- [A Novel Approach to Explainable AI with Quantized Active Ingredients in Decision Making](https://arxiv.org/abs/2601.08733)

## 커뮤니티

- [What happened to 1.58bit LLMs?](https://www.reddit.com/r/LocalLLaMA/comments/1qcj1lr/what_happened_to_158bit_llms/)
- [Cowork: Claude Code for the rest of your work](https://claude.com/blog/cowork-research-preview)
- [FOSS in times of war, scarcity and (adversarial) AI [video]](https://fosdem.org/2026/schedule/event/FE7ULY-foss-in-times-of-war-scarcity-and-ai/)
- [[D] Is anyone actually paying for GPU Cluster TCO Consulting? (Because most companies are overpaying by 20%+)](https://www.reddit.com/r/MachineLearning/comments/1qbljgq/d_is_anyone_actually_paying_for_gpu_cluster_tco/)
- [FBI raids Washington Post reporter's home](https://www.theguardian.com/us-news/2026/jan/14/fbi-raid-washington-post-hannah-natanson)
- [Zhipu AI breaks US chip reliance with first major model trained on Huawei stack (GLM-Image)](https://www.reddit.com/r/LocalLLaMA/comments/1qd6nho/zhipu_ai_breaks_us_chip_reliance_with_first_major/)

---

## 링크드인 포스트 초안

```
오늘의 AI 하이라이트

- Information Capacity: Evaluating the Efficiency of Large Language Models via Text Compression
- ROSS: RObust decentralized Stochastic learning based on Shapley values
- Stop treating LLM context as a linear chat: We need a Context-Editing IDE for serious engineering and professional project development

#AI #Tech #LLM
```
