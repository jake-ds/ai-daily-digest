"""ë§í¬ë“œì¸ ì½˜í…ì¸  ì „ë¬¸ê°€ Agent - ê¸€ê° ì„ ì • ì „ë¬¸"""

import os
import json
from typing import TYPE_CHECKING, Optional
from dataclasses import dataclass

try:
    from anthropic import Anthropic
except ImportError:
    Anthropic = None

if TYPE_CHECKING:
    from ..collectors.rss_collector import Article


@dataclass
class LinkedInCandidate:
    """ë§í¬ë“œì¸ ê¸€ê° í›„ë³´"""
    article_url: str
    score: float          # 0-10
    verdict: str          # "ì¶”ì²œ", "ë³´ë¥˜", "íƒˆë½"
    reason: str           # í•œ ì¤„ ì´ìœ 
    angle: str            # ì¶”ì²œ ê°ë„
    hook: str             # ì˜¤í”„ë‹ í›… ì•„ì´ë””ì–´


class LinkedInExpert:
    """ë§í¬ë“œì¸ ì½˜í…ì¸  íë ˆì´í„° - ê¸€ê° ì„ ì • ì „ë¬¸ê°€

    ì´ˆê¸° ë‹¨ê³„ì—ì„œ ëŒ€ëŸ‰ì˜ ê¸°ì‚¬ë¥¼ ë¹ ë¥´ê²Œ ìŠ¤í¬ë¦¬ë‹í•˜ì—¬
    ë§í¬ë“œì¸ì— ì í•©í•œ ê¸€ê°ì„ ì„ ë³„í•©ë‹ˆë‹¤.
    """

    # 1ì°¨ ìŠ¤í¬ë¦¬ë‹: ë¹ ë¥¸ í•„í„°ë§ (ë°°ì¹˜ ì²˜ë¦¬)
    SCREENING_PROMPT = """ë‹¹ì‹ ì€ ë§í¬ë“œì¸ ì½˜í…ì¸  íë ˆì´í„°ì…ë‹ˆë‹¤.
AI/Tech ì—…ê³„ ì¢…ì‚¬ìë“¤ì´ ê´€ì‹¬ ê°€ì§ˆ ë§Œí•œ ê¸€ê°ì„ ì°¾ì•„ì•¼ í•©ë‹ˆë‹¤.

## ë‹¹ì‹ ì˜ ê¸°ì¤€
- ë¹…í…Œí¬ì˜ ìƒˆ ê¸°ëŠ¥/ë°œí‘œë³´ë‹¤ "ì™œ ì´ê²Œ ë‚˜ì™”ëŠ”ì§€" ë§¥ë½ì´ ìˆëŠ” ê²ƒ
- ë…¼ë¬¸ ì¤‘ì—ì„œë„ ì‹¤ë¬´ìê°€ ì´í•´í•˜ê³  ì ìš©í•  ìˆ˜ ìˆëŠ” ê²ƒ
- ì—…ê³„ íŠ¸ë Œë“œë¥¼ ë³´ì—¬ì£¼ëŠ” ìˆ«ìë‚˜ ì‚¬ë¡€ê°€ ìˆëŠ” ê²ƒ
- ë…¼ìŸê±°ë¦¬ê°€ ìˆì–´ì„œ ëŒ“ê¸€ì´ ë‹¬ë¦´ ë§Œí•œ ê²ƒ
- "ê·¸ë˜ì„œ ë­?" í•˜ê³  ë„˜ê¸°ì§€ ì•Šì„ ë§Œí•œ ê²ƒ

## íƒˆë½ ê¸°ì¤€ (ì´ëŸ° ê±´ ë°”ë¡œ íƒˆë½)
- ë‹¨ìˆœ ì œí’ˆ ì¶œì‹œ ë°œí‘œ (ìƒˆ ë²„ì „, ìƒˆ ê¸°ëŠ¥)
- ë„ˆë¬´ ê¸°ìˆ ì ì´ë¼ ì„¤ëª…ì´ ì–´ë ¤ìš´ ë…¼ë¬¸
- ì´ë¯¸ ë§ì´ ì•Œë ¤ì§„ ë»”í•œ ë‚´ìš©
- í•œêµ­ ë…ìê°€ ê´€ì‹¬ ì—†ì„ ì§€ì—­ ë‰´ìŠ¤
- ìŠ¤ìº”ë“¤/ê°€ì‹­ì„± ê¸°ì‚¬ (ë‹¨, ì—…ê³„ ì‹œì‚¬ì ì´ ìˆìœ¼ë©´ ì˜ˆì™¸)

## í‰ê°€í•  ê¸°ì‚¬ë“¤
{articles}

## ì‘ë‹µ í˜•ì‹ (JSON ë°°ì—´)
ê° ê¸°ì‚¬ì— ëŒ€í•´:
```json
[
  {{
    "index": 0,
    "score": 7.5,
    "verdict": "ì¶”ì²œ",
    "reason": "í•œ ì¤„ë¡œ ì™œ ì¶”ì²œ/íƒˆë½ì¸ì§€"
  }},
  ...
]
```

verdictëŠ” "ì¶”ì²œ"(7ì  ì´ìƒ), "ë³´ë¥˜"(5-7ì ), "íƒˆë½"(5ì  ë¯¸ë§Œ) ì¤‘ í•˜ë‚˜.
JSONë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”."""

    # 2ì°¨ ì‹¬ì¸µ í‰ê°€: ìƒìœ„ í›„ë³´ ì •ë°€ ë¶„ì„
    DEEP_EVAL_PROMPT = """ë‹¹ì‹ ì€ ë§í¬ë“œì¸ ì½˜í…ì¸  ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì´ ê¸°ì‚¬ê°€ ë§í¬ë“œì¸ í¬ìŠ¤íŠ¸ë¡œ ì–¼ë§ˆë‚˜ ì¢‹ì€ ê¸€ê°ì¸ì§€ ì‹¬ì¸µ í‰ê°€í•´ì£¼ì„¸ìš”.

## ê¸°ì‚¬ ì •ë³´
ì œëª©: {title}
ì¶œì²˜: {source}
ì¹´í…Œê³ ë¦¬: {category}
ìš”ì•½: {summary}

## í‰ê°€ ê¸°ì¤€

### 1. í† ë¡  ìœ ë°œë ¥ (0-10)
- ì‚¬ëŒë“¤ì´ ìê¸° ê²½í—˜/ì˜ê²¬ì„ ëŒ“ê¸€ë¡œ ë‹¬ê³  ì‹¶ì–´ì§ˆê¹Œ?
- "ë‚˜ë„ ê·¸ëŸ° ê²½í—˜ ìˆì–´", "ë‚˜ëŠ” ë‹¤ë¥´ê²Œ ìƒê°í•´" ë°˜ì‘ì´ ë‚˜ì˜¬ê¹Œ?

### 2. ì„¤ëª… ê°€ëŠ¥ì„± (0-10)
- ë¹„ì „ë¬¸ê°€ì—ê²Œ ì‰½ê²Œ ì„¤ëª…í•  ìˆ˜ ìˆëŠ” ì£¼ì œì¸ê°€?
- ë³µì¡í•œ ë°°ê²½ì§€ì‹ ì—†ì´ í•µì‹¬ì„ ì „ë‹¬í•  ìˆ˜ ìˆë‚˜?

### 3. ì‹œì˜ì„± (0-10) - ë°ì¼ë¦¬ í¬ìŠ¤íŠ¸ì˜ í•µì‹¬!
- ì˜¤ëŠ˜/ì´ë²ˆ ì£¼ì— ë‚˜ì˜¨ ë‰´ìŠ¤ì¸ê°€? (ìµœê·¼ 48ì‹œê°„ì´ë©´ ê°€ì‚°ì )
- ë‹¤ë¥¸ ë§¤ì²´ì—ì„œë„ ë‹¤ë£¨ê³  ìˆëŠ” í™”ì œì¸ê°€?
- "ì§€ê¸ˆ ì•ˆ ì˜¬ë¦¬ë©´ ëŠ¦ëŠ”ë‹¤"ëŠ” ëŠë‚Œì´ ìˆë‚˜?

ì ìˆ˜ ê¸°ì¤€:
- 9-10: ì˜¤ëŠ˜ í„°ì§„ ë¹…ë‰´ìŠ¤
- 7-8: ì´ë²ˆ ì£¼ í•«ì´ìŠˆ
- 5-6: ê´€ì‹¬ ê°€ëŠ” ì£¼ì œì§€ë§Œ ê¸‰í•˜ì§€ ì•ŠìŒ
- 3-4: evergreen ì½˜í…ì¸ 

### 4. ë…ì°½ì  ê°ë„ (0-10)
- ë»”í•˜ì§€ ì•Šì€ ê´€ì ìœ¼ë¡œ í’€ì–´ë‚¼ ìˆ˜ ìˆë‚˜?
- ë‚¨ë“¤ì´ ì•ˆ í•˜ëŠ” ì´ì•¼ê¸°ë¥¼ í•  ìˆ˜ ìˆë‚˜?

### 5. ê³µìœ  ìš•êµ¬ (0-10)
- ì½ê³  ë‚˜ì„œ ë™ë£Œì—ê²Œ "ì´ê±° ë´¤ì–´?" í•˜ê³  ë³´ë‚´ê³  ì‹¶ë‚˜?
- ë‚´ í”„ë¡œí•„ì— ì˜¬ë ¤ì„œ ì „ë¬¸ì„±ì„ ë³´ì—¬ì£¼ê³  ì‹¶ë‚˜?

## Hook ì‘ì„± ê°€ì´ë“œ (ë§¤ìš° ì¤‘ìš”)
LinkedInì—ì„œ ì²« 3ì¤„ì´ "ë”ë³´ê¸°" ì „ì— ë³´ì…ë‹ˆë‹¤. ìŠ¤í¬ë¡¤ì„ ë©ˆì¶”ê²Œ í•˜ëŠ” Hookì´ í•„ìš”í•©ë‹ˆë‹¤.

íš¨ê³¼ì ì¸ Hook íŒ¨í„´ (ì´ ì¤‘ í•˜ë‚˜ ì‚¬ìš©):
1. ìˆ«ìë¡œ ì‹œì‘: "M4 Maxì—ì„œ ì´ˆë‹¹ 464 í† í°." / "3ì¼ ë§Œì— 10ë§Œ ë‹¤ìš´ë¡œë“œ."
2. ì˜ì™¸ì„±/ì§ˆë¬¸: "AIì—ê²Œ SSH ê¶Œí•œ ì¤˜ë„ ë ê¹Œ?" / "ì™œ ë‹¤ë“¤ RAGë¥¼ ë²„ë¦¬ê³  ìˆì„ê¹Œ?"
3. ëŒ€ë¹„: "ê¸°ì¡´ì—ëŠ” 3ì‹œê°„. ì´ì œëŠ” 3ë¶„."
4. ìƒí™© ê³µê°: "íŒ€ë§ˆë‹¤ ë‹¤ë¥¸ AI ë„êµ¬, ë‹¤ë¥¸ ë°©ì‹. í˜¹ì‹œ ì´ëŸ° ê³ ë¯¼ ìˆìœ¼ì‹ ê°€ìš”?"
5. ì§ì ‘ì  ë°œê²¬: "í¥ë¯¸ë¡œìš´ ì—°êµ¬ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤."

í”¼í•´ì•¼ í•  Hook (ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€):
- âŒ "AI ì‹œëŒ€, ~ì˜ í™œìš©ë²•" (2015ë…„ ì œëª© ê°™ìŒ)
- âŒ "~ê°€ ì£¼ëª©ë©ë‹ˆë‹¤", "~ê°€ í™”ì œì…ë‹ˆë‹¤" (í”¼ë™í˜•)
- âŒ "ìƒˆë¡œìš´ ì§€í‰ì„ ì—´ë‹¤", "íŒ¨ëŸ¬ë‹¤ì„ ì „í™˜" (ê±°ì°½í•¨)
- âŒ "ì˜¤ëŠ˜ì€ ~ì— ëŒ€í•´ ì´ì•¼ê¸°í•´ë³´ê² ìŠµë‹ˆë‹¤" (ì§€ë£¨í•¨)

## ì‘ë‹µ í˜•ì‹ (JSON)
```json
{{
  "discussion_trigger": 8,
  "explainability": 7,
  "timeliness": 6,
  "unique_angle": 7,
  "shareability": 8,
  "total_score": 7.2,
  "verdict": "ì¶”ì²œ",
  "angle": "ì´ ê¸°ì‚¬ë¥¼ ì–´ë–¤ ê´€ì /ì§ˆë¬¸ìœ¼ë¡œ í’€ì–´ê°ˆì§€ (1ë¬¸ì¥)",
  "hook": "ìœ„ íŒ¨í„´ ì¤‘ í•˜ë‚˜ë¡œ ì‘ì„±í•œ Hook (1-2ë¬¸ì¥, êµ¬ì²´ì  ìˆ«ìë‚˜ ì§ˆë¬¸ í¬í•¨)",
  "reason": "ì™œ ì´ ê¸€ê°ì´ ì¢‹ì€ì§€/ì•„ì‰¬ìš´ì§€ ì†”ì§í•˜ê²Œ (1-2ë¬¸ì¥)"
}}
```

JSONë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”."""

    # ê¸°ì‚¬ ê·¸ë£¹í•‘: ì—°ê´€ ì£¼ì œ ë¶„ì„
    GROUPING_PROMPT = """ë‹¹ì‹ ì€ AI/Tech ë‰´ìŠ¤ ë¶„ì„ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ ê¸°ì‚¬ë“¤ì„ ì—°ê´€ ì£¼ì œë³„ë¡œ ê·¸ë£¹í•‘í•´ì£¼ì„¸ìš”.

## ê¸°ì‚¬ ëª©ë¡
{articles}

## ê·¸ë£¹í•‘ ê¸°ì¤€
- ê°™ì€ ê¸°ìˆ  ë¶„ì•¼ (ì˜ˆ: LLM, Agent, Vision)
- ê°™ì€ íšŒì‚¬/ì¡°ì§ ê´€ë ¨
- ê°™ì€ íŠ¸ë Œë“œ/íë¦„ (ì˜ˆ: ì˜¤í”ˆì†ŒìŠ¤ ê²½ìŸ, ëª¨ë¸ ê²½ëŸ‰í™”)
- ê°™ì€ ë¬¸ì œ/ê³¼ì œ (ì˜ˆ: ë¹„ìš© ì ˆê°, ì„±ëŠ¥ ê°œì„ )

## ì‘ë‹µ í˜•ì‹ (JSON)
```json
{{
  "groups": [
    {{
      "theme": "ê·¸ë£¹ ì£¼ì œ (ì˜ˆ: 'AI Agent ê²½ìŸ ì‹¬í™”')",
      "keyword": "í•µì‹¬ í‚¤ì›Œë“œ (ì˜ˆ: 'Agent')",
      "article_indices": [0, 2, 5],
      "connection": "ì´ ê¸°ì‚¬ë“¤ì˜ ì—°ê²°ê³ ë¦¬ ì„¤ëª… (1ë¬¸ì¥)"
    }},
    ...
  ],
  "ungrouped": [1, 3]
}}
```

ê·œì¹™:
- ìµœì†Œ 3ê°œ ì´ìƒ ê¸°ì‚¬ê°€ ë¬¶ì—¬ì•¼ ê·¸ë£¹ìœ¼ë¡œ ì¸ì •
- 2ê°œ ì´í•˜ëŠ” ungroupedë¡œ ë¶„ë¥˜
- í•˜ë‚˜ì˜ ê¸°ì‚¬ëŠ” í•˜ë‚˜ì˜ ê·¸ë£¹ì—ë§Œ ì†í•¨
- ê°€ì¥ í° ê·¸ë£¹ì„ ì²« ë²ˆì§¸ë¡œ ë°°ì¹˜

JSONë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”."""

    def __init__(self):
        self.client = None
        if Anthropic and os.getenv("ANTHROPIC_API_KEY"):
            self.client = Anthropic()

    def _format_articles_for_screening(self, articles: list["Article"]) -> str:
        """ìŠ¤í¬ë¦¬ë‹ìš© ê¸°ì‚¬ ëª©ë¡ í¬ë§·"""
        lines = []
        for i, article in enumerate(articles):
            summary = article.ai_summary or article.summary or ""
            summary = summary[:200].replace("\n", " ")
            lines.append(f"[{i}] {article.title}")
            lines.append(f"    ì¶œì²˜: {article.source} | ì¹´í…Œê³ ë¦¬: {article.category}")
            lines.append(f"    ìš”ì•½: {summary}")
            lines.append("")
        return "\n".join(lines)

    def screen_articles(
        self,
        articles: list["Article"],
        batch_size: int = 15
    ) -> list[tuple["Article", dict]]:
        """1ì°¨ ìŠ¤í¬ë¦¬ë‹: ëŒ€ëŸ‰ ê¸°ì‚¬ ë¹ ë¥´ê²Œ í•„í„°ë§

        Args:
            articles: ìŠ¤í¬ë¦¬ë‹í•  ê¸°ì‚¬ ëª©ë¡
            batch_size: í•œ ë²ˆì— ì²˜ë¦¬í•  ê¸°ì‚¬ ìˆ˜

        Returns:
            (ê¸°ì‚¬, í‰ê°€ê²°ê³¼) íŠœí”Œ ë¦¬ìŠ¤íŠ¸ (ì ìˆ˜ìˆœ ì •ë ¬)
        """
        if not self.client:
            print("  LinkedIn Expert: API í‚¤ ì—†ìŒ, ìŠ¤í‚µ")
            return [(a, {"score": 5, "verdict": "ë³´ë¥˜", "reason": "API ì—†ìŒ"}) for a in articles]

        all_results = []

        # ë°°ì¹˜ ì²˜ë¦¬
        for i in range(0, len(articles), batch_size):
            batch = articles[i:i+batch_size]
            batch_text = self._format_articles_for_screening(batch)

            prompt = self.SCREENING_PROMPT.format(articles=batch_text)

            try:
                response = self.client.messages.create(
                    model="claude-3-5-haiku-20241022",
                    max_tokens=2000,
                    messages=[{"role": "user", "content": prompt}]
                )

                result_text = response.content[0].text.strip()

                # JSON íŒŒì‹±
                if "```json" in result_text:
                    result_text = result_text.split("```json")[1].split("```")[0]
                elif "```" in result_text:
                    result_text = result_text.split("```")[1].split("```")[0]

                evaluations = json.loads(result_text.strip())

                # ê²°ê³¼ ë§¤í•‘
                for eval_item in evaluations:
                    idx = eval_item.get("index", 0)
                    if idx < len(batch):
                        all_results.append((batch[idx], eval_item))

            except Exception as e:
                print(f"  ìŠ¤í¬ë¦¬ë‹ ë°°ì¹˜ ì‹¤íŒ¨: {e}")
                # ì‹¤íŒ¨í•œ ë°°ì¹˜ëŠ” ê¸°ë³¸ ì ìˆ˜ë¡œ
                for article in batch:
                    all_results.append((article, {
                        "score": 5,
                        "verdict": "ë³´ë¥˜",
                        "reason": "í‰ê°€ ì‹¤íŒ¨"
                    }))

        # ì ìˆ˜ìˆœ ì •ë ¬
        all_results.sort(key=lambda x: x[1].get("score", 0), reverse=True)

        return all_results

    # í‰ê°€ ê°€ì¤‘ì¹˜
    EVAL_WEIGHTS = {
        "timeliness": 2.5,        # ì‹œì˜ì„± 2.5ë°° ê°•í™”
        "discussion_trigger": 1.5,
        "shareability": 1.5,
        "explainability": 1.0,
        "unique_angle": 1.0,
    }

    def deep_evaluate(self, article: "Article") -> Optional[LinkedInCandidate]:
        """2ì°¨ ì‹¬ì¸µ í‰ê°€: ê°œë³„ ê¸°ì‚¬ ì •ë°€ ë¶„ì„"""
        if not self.client:
            return None

        prompt = self.DEEP_EVAL_PROMPT.format(
            title=article.title,
            source=article.source,
            category=article.category,
            summary=article.ai_summary or article.summary or "ìš”ì•½ ì—†ìŒ"
        )

        try:
            response = self.client.messages.create(
                model="claude-3-5-haiku-20241022",
                max_tokens=800,
                messages=[{"role": "user", "content": prompt}]
            )

            result_text = response.content[0].text.strip()

            # JSON íŒŒì‹±
            if "```json" in result_text:
                result_text = result_text.split("```json")[1].split("```")[0]
            elif "```" in result_text:
                result_text = result_text.split("```")[1].split("```")[0]

            data = json.loads(result_text.strip())

            # ê°€ì¤‘ì¹˜ ì ìš©ëœ ì ìˆ˜ ê³„ì‚°
            weighted_score = self._calculate_weighted_score(data)

            return LinkedInCandidate(
                article_url=article.url,
                score=weighted_score,
                verdict=data.get("verdict", "ë³´ë¥˜"),
                reason=data.get("reason", ""),
                angle=data.get("angle", ""),
                hook=data.get("hook", "")
            )

        except Exception as e:
            print(f"  ì‹¬ì¸µí‰ê°€ ì‹¤íŒ¨ [{article.title[:30]}]: {e}")
            return None

    def _calculate_weighted_score(self, data: dict) -> float:
        """ê°€ì¤‘ì¹˜ ì ìš©ëœ ì ìˆ˜ ê³„ì‚°"""
        scores = {
            "timeliness": data.get("timeliness", 5),
            "discussion_trigger": data.get("discussion_trigger", 5),
            "shareability": data.get("shareability", 5),
            "explainability": data.get("explainability", 5),
            "unique_angle": data.get("unique_angle", 5),
        }

        total_weight = sum(self.EVAL_WEIGHTS.values())
        weighted_sum = sum(
            scores[key] * self.EVAL_WEIGHTS[key]
            for key in scores
        )

        return round(weighted_sum / total_weight, 1)

    def group_articles_by_theme(
        self,
        candidates: list[tuple["Article", "LinkedInCandidate"]],
        min_group_size: int = 3
    ) -> list[list[tuple["Article", "LinkedInCandidate"]]]:
        """ì—°ê´€ ì£¼ì œë³„ ê¸°ì‚¬ ê·¸ë£¹í•‘

        Args:
            candidates: (ê¸°ì‚¬, í‰ê°€ê²°ê³¼) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
            min_group_size: ê·¸ë£¹ìœ¼ë¡œ ì¸ì •í•  ìµœì†Œ ê¸°ì‚¬ ìˆ˜

        Returns:
            ê·¸ë£¹ë³„ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ (í¬ê¸° ìˆœ ì •ë ¬)
        """
        if not self.client or len(candidates) < min_group_size:
            return [candidates]

        # ê¸°ì‚¬ ëª©ë¡ í¬ë§·
        articles_text = []
        for i, (article, _) in enumerate(candidates):
            summary = article.ai_summary or article.summary or ""
            summary = summary[:150].replace("\n", " ")
            articles_text.append(f"[{i}] {article.title}")
            articles_text.append(f"    ìš”ì•½: {summary}")
            articles_text.append("")

        prompt = self.GROUPING_PROMPT.format(articles="\n".join(articles_text))

        try:
            response = self.client.messages.create(
                model="claude-3-5-haiku-20241022",
                max_tokens=1000,
                messages=[{"role": "user", "content": prompt}]
            )

            result_text = response.content[0].text.strip()

            # JSON íŒŒì‹±
            if "```json" in result_text:
                result_text = result_text.split("```json")[1].split("```")[0]
            elif "```" in result_text:
                result_text = result_text.split("```")[1].split("```")[0]

            data = json.loads(result_text.strip())

            # ê·¸ë£¹ë³„ ê¸°ì‚¬ ë§¤í•‘
            groups = []
            used_indices = set()

            for group_info in data.get("groups", []):
                indices = group_info.get("article_indices", [])
                if len(indices) >= min_group_size:
                    group = []
                    for idx in indices:
                        if idx < len(candidates) and idx not in used_indices:
                            group.append(candidates[idx])
                            used_indices.add(idx)
                    if len(group) >= min_group_size:
                        # ê·¸ë£¹ì— í…Œë§ˆ ì •ë³´ ì €ì¥ (ì²« ë²ˆì§¸ ì•„ì´í…œì˜ candidateì—)
                        groups.append((group, group_info.get("keyword", "")))

            # ê·¸ë£¹ì— ì†í•˜ì§€ ì•Šì€ ê¸°ì‚¬ë“¤
            ungrouped = [
                candidates[i] for i in range(len(candidates))
                if i not in used_indices
            ]

            # í¬ê¸° ìˆœ ì •ë ¬ (í° ê·¸ë£¹ ë¨¼ì €)
            groups.sort(key=lambda x: len(x[0]), reverse=True)

            # ê²°ê³¼ ë°˜í™˜ (ê·¸ë£¹ë§Œ, í‚¤ì›Œë“œ ì •ë³´ëŠ” ë³„ë„ ì €ì¥)
            result = []
            self._group_keywords = {}  # ê·¸ë£¹ë³„ í‚¤ì›Œë“œ ì €ì¥
            for i, (group, keyword) in enumerate(groups):
                result.append(group)
                self._group_keywords[i] = keyword

            if ungrouped:
                result.append(ungrouped)
                self._group_keywords[len(result) - 1] = ""

            print(f"   ê·¸ë£¹í•‘ ì™„ë£Œ: {len(groups)}ê°œ ê·¸ë£¹, {len(ungrouped)}ê°œ ë¯¸ë¶„ë¥˜")
            for i, (group, keyword) in enumerate(groups):
                print(f"     ê·¸ë£¹ {i+1} ({keyword}): {len(group)}ê°œ ê¸°ì‚¬")

            return result

        except Exception as e:
            print(f"   ê·¸ë£¹í•‘ ì‹¤íŒ¨: {e}")
            return [candidates]

    def get_group_keyword(self, group_index: int) -> str:
        """ê·¸ë£¹ì˜ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ë°˜í™˜"""
        return getattr(self, '_group_keywords', {}).get(group_index, "")

    def curate_linkedin_candidates(
        self,
        articles: list["Article"],
        screen_top_n: int = 50,
        final_top_n: int = 5,
        ensure_diversity: bool = True
    ) -> list[tuple["Article", LinkedInCandidate]]:
        """ë§í¬ë“œì¸ ê¸€ê° íë ˆì´ì…˜ ì „ì²´ íŒŒì´í”„ë¼ì¸

        Args:
            articles: ì „ì²´ ê¸°ì‚¬ ëª©ë¡
            screen_top_n: 1ì°¨ ìŠ¤í¬ë¦¬ë‹ ëŒ€ìƒ ìˆ˜
            final_top_n: ìµœì¢… ì„ ì • ìˆ˜
            ensure_diversity: ì¹´í…Œê³ ë¦¬ ë‹¤ì–‘ì„± ë³´ì¥ ì—¬ë¶€

        Returns:
            (ê¸°ì‚¬, í‰ê°€ê²°ê³¼) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        print(f"\nğŸ“‹ LinkedIn Expert: ê¸€ê° íë ˆì´ì…˜ ì‹œì‘")
        print(f"   ëŒ€ìƒ: {len(articles)}ê°œ â†’ ìŠ¤í¬ë¦¬ë‹: {screen_top_n}ê°œ â†’ ìµœì¢…: {final_top_n}ê°œ")

        # 1ë‹¨ê³„: ìŠ¤í¬ë¦¬ë‹ ëŒ€ìƒ ì„ ì • (í‚¤ì›Œë“œ ì ìˆ˜ ìƒìœ„ + ì¹´í…Œê³ ë¦¬ ë‹¤ì–‘ì„±)
        screening_pool = self._select_screening_pool(articles, screen_top_n)
        print(f"   1ì°¨ í’€ ì„ ì •: {len(screening_pool)}ê°œ")

        # 2ë‹¨ê³„: 1ì°¨ ìŠ¤í¬ë¦¬ë‹
        print(f"   1ì°¨ ìŠ¤í¬ë¦¬ë‹ ì¤‘...")
        screened = self.screen_articles(screening_pool)

        # "ì¶”ì²œ" ë˜ëŠ” "ë³´ë¥˜" ì¤‘ ìƒìœ„ ì ìˆ˜ ê¸°ì‚¬ ì„ ë³„ (ê¸°ì¤€ ì™„í™”)
        recommended = [
            (a, e) for a, e in screened
            if e.get("verdict") in ["ì¶”ì²œ", "ë³´ë¥˜"] or e.get("score", 0) >= 5.5
        ]
        print(f"   1ì°¨ í†µê³¼: {len(recommended)}ê°œ")

        # 3ë‹¨ê³„: ë‹¤ì–‘ì„± ë³´ì¥í•˜ë©° ì‹¬ì¸µ í‰ê°€ ëŒ€ìƒ ì„ ì •
        if ensure_diversity:
            deep_eval_targets = self._ensure_diversity(recommended, final_top_n * 2)
        else:
            deep_eval_targets = [a for a, _ in recommended[:final_top_n * 2]]

        # 4ë‹¨ê³„: ì‹¬ì¸µ í‰ê°€
        print(f"   2ì°¨ ì‹¬ì¸µ í‰ê°€ ì¤‘... ({len(deep_eval_targets)}ê°œ)")
        final_candidates = []
        news_categories = {"bigtech", "vc", "news", "community", "korean"}

        for article in deep_eval_targets:
            candidate = self.deep_evaluate(article)
            if candidate:
                # ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ëŠ” ì¢€ ë” ê´€ëŒ€í•˜ê²Œ (ë‹¤ì–‘ì„± í™•ë³´)
                if article.category in news_categories:
                    # íƒˆë½ì´ì–´ë„ ì ìˆ˜ í˜ë„í‹°ë§Œ ì£¼ê³  í¬í•¨
                    if candidate.verdict == "íƒˆë½":
                        candidate.score = max(4.0, candidate.score - 2)
                    final_candidates.append((article, candidate))
                else:
                    # researchëŠ” ì¶”ì²œ/ë³´ë¥˜ë§Œ
                    if candidate.verdict in ["ì¶”ì²œ", "ë³´ë¥˜"]:
                        final_candidates.append((article, candidate))

        # ì ìˆ˜ìˆœ ì •ë ¬
        final_candidates.sort(key=lambda x: x[1].score, reverse=True)

        # ìµœì¢… ë‹¤ì–‘ì„± ë³´ì¥
        if ensure_diversity:
            final_candidates = self._final_diversity_filter(
                final_candidates, final_top_n
            )
        else:
            final_candidates = final_candidates[:final_top_n]

        print(f"   âœ… ìµœì¢… ì„ ì •: {len(final_candidates)}ê°œ")

        return final_candidates

    def _select_screening_pool(
        self,
        articles: list["Article"],
        n: int
    ) -> list["Article"]:
        """ìŠ¤í¬ë¦¬ë‹ ëŒ€ìƒ ì„ ì • (ì¹´í…Œê³ ë¦¬ ë‹¤ì–‘ì„± ê³ ë ¤)"""
        from collections import defaultdict

        # ì¹´í…Œê³ ë¦¬ë³„ ê·¸ë£¹í™”
        by_category = defaultdict(list)
        for article in articles:
            by_category[article.category].append(article)

        # ê° ì¹´í…Œê³ ë¦¬ì—ì„œ ì ìˆ˜ìˆœ ì •ë ¬
        for cat in by_category:
            by_category[cat].sort(key=lambda x: x.score, reverse=True)

        # ì¹´í…Œê³ ë¦¬ë³„ ìµœì†Œ í• ë‹¹
        min_per_category = {
            "bigtech": 8,
            "vc": 8,
            "news": 10,
            "research": 10,
            "community": 8,
            "korean": 8,
        }

        selected = []
        used_urls = set()

        # 1ë‹¨ê³„: ì¹´í…Œê³ ë¦¬ë³„ ìµœì†Œ í• ë‹¹
        for cat, min_count in min_per_category.items():
            for article in by_category.get(cat, [])[:min_count]:
                if article.url not in used_urls:
                    selected.append(article)
                    used_urls.add(article.url)

        # 2ë‹¨ê³„: ë‚˜ë¨¸ì§€ëŠ” ì ìˆ˜ìˆœ
        all_sorted = sorted(articles, key=lambda x: x.score, reverse=True)
        for article in all_sorted:
            if len(selected) >= n:
                break
            if article.url not in used_urls:
                selected.append(article)
                used_urls.add(article.url)

        return selected

    def _ensure_diversity(
        self,
        candidates: list[tuple["Article", dict]],
        n: int
    ) -> list["Article"]:
        """ì‹¬ì¸µ í‰ê°€ ëŒ€ìƒ ë‹¤ì–‘ì„± ë³´ì¥

        research ìµœì†Œ 3ê°œ, news ìµœì†Œ 3ê°œ í¬í•¨
        """
        # ë‰´ìŠ¤ ê·¸ë£¹ (research ì œì™¸)
        news_categories = {"bigtech", "vc", "news", "community", "korean"}

        selected = []
        used_urls = set()

        # research ìµœì†Œ 3ê°œ
        research_count = 0
        for article, _ in candidates:
            if article.category == "research" and research_count < 3:
                selected.append(article)
                used_urls.add(article.url)
                research_count += 1

        # news ìµœì†Œ 3ê°œ (ë” ì ê·¹ì ìœ¼ë¡œ)
        news_count = 0
        for article, _ in candidates:
            if article.category in news_categories and news_count < 3:
                if article.url not in used_urls:
                    selected.append(article)
                    used_urls.add(article.url)
                    news_count += 1

        print(f"   ë‹¤ì–‘ì„± ë³´ì¥: research {research_count}ê°œ, news {news_count}ê°œ")

        # ë‚˜ë¨¸ì§€ëŠ” ì ìˆ˜ìˆœ
        for article, _ in candidates:
            if len(selected) >= n:
                break
            if article.url not in used_urls:
                selected.append(article)
                used_urls.add(article.url)

        return selected

    def _final_diversity_filter(
        self,
        candidates: list[tuple["Article", "LinkedInCandidate"]],
        n: int
    ) -> list[tuple["Article", "LinkedInCandidate"]]:
        """ìµœì¢… ì„ ì • ë‹¤ì–‘ì„± í•„í„°

        ìµœì†Œ 1ê°œ research, ìµœì†Œ 1ê°œ news ë³´ì¥
        """
        news_categories = {"bigtech", "vc", "news", "community", "korean"}

        selected = []
        has_research = False
        has_news = False
        used_urls = set()

        # 1ë‹¨ê³„: research 1ê°œ ë³´ì¥ (ì ìˆ˜ ìƒê´€ì—†ì´)
        for article, candidate in candidates:
            if article.category == "research" and not has_research:
                selected.append((article, candidate))
                used_urls.add(article.url)
                has_research = True
                break

        # 2ë‹¨ê³„: news 1ê°œ ë³´ì¥ (ì ìˆ˜ ìƒê´€ì—†ì´)
        for article, candidate in candidates:
            if article.category in news_categories and not has_news:
                if article.url not in used_urls:
                    selected.append((article, candidate))
                    used_urls.add(article.url)
                    has_news = True
                    break

        # ë‰´ìŠ¤ê°€ candidatesì— ì—†ìœ¼ë©´ ê²½ê³ 
        if not has_news:
            print(f"   âš ï¸ ê²½ê³ : ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ í›„ë³´ê°€ ì—†ìŠµë‹ˆë‹¤")

        # 3ë‹¨ê³„: ë‚˜ë¨¸ì§€ ì ìˆ˜ìˆœ
        for article, candidate in candidates:
            if len(selected) >= n:
                break
            if article.url not in used_urls:
                selected.append((article, candidate))
                used_urls.add(article.url)

        # ì ìˆ˜ìˆœ ì¬ì •ë ¬
        selected.sort(key=lambda x: x[1].score, reverse=True)

        return selected

    def print_curation_report(
        self,
        candidates: list[tuple["Article", "LinkedInCandidate"]]
    ):
        """íë ˆì´ì…˜ ê²°ê³¼ ë¦¬í¬íŠ¸"""
        print("\n" + "=" * 60)
        print("ğŸ“Š LinkedIn Expert íë ˆì´ì…˜ ë¦¬í¬íŠ¸")
        print("=" * 60)

        for i, (article, candidate) in enumerate(candidates, 1):
            print(f"\nğŸ¯ #{i} (ì ìˆ˜: {candidate.score}/10) [{candidate.verdict}]")
            print(f"ì œëª©: {article.title[:55]}...")
            print(f"ì¹´í…Œê³ ë¦¬: {article.category} | ì¶œì²˜: {article.source}")
            print(f"â”œâ”€ ì´ìœ : {candidate.reason[:60]}...")
            print(f"â”œâ”€ ê°ë„: {candidate.angle[:60]}...")
            print(f"â””â”€ í›…: {candidate.hook[:60]}...")

        print("\n" + "=" * 60)
